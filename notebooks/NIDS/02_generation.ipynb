{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://192.168.1.20:1318/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "USE_MODEL = \"pixtral\"\n",
    "MODEL_NAME_DICT = {\"qwen\": \"Qwen/Qwen2-VL-72B-Instruct\",\n",
    "                   \"pixtral\": \"mistralai/Pixtral-Large-Instruct-2411\"}\n",
    "MODEL_NAME = MODEL_NAME_DICT[USE_MODEL]\n",
    "SYSTEM_PROMPT_DICT = {\"qwen\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\",\n",
    "                      \"pixtral\": \"You are a helpful assistant.\"} # TODO : pixtral 프롬프트 지정 필요\n",
    "SYSTEM_PROMPT = SYSTEM_PROMPT_DICT[USE_MODEL]\n",
    "def LLM_Call(prompt:str):\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "        ],\n",
    "        temperature=0.001,\n",
    "        top_p=0.001,\n",
    "        max_tokens=4096,\n",
    "        extra_body={\n",
    "            \"repetition_penalty\": 1.03,\n",
    "        },\n",
    "    )\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def extract_data_dict(result):\n",
    "    pattern = re.compile(r\"(?<=python\\n)\\{[\\s\\S]*?\\}\")\n",
    "    match = pattern.search(result)\n",
    "    if match:\n",
    "        data_str = match.group(0)\n",
    "        data_dict = ast.literal_eval(data_str)\n",
    "        return data_dict\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_attack_type_samples(df:pd.DataFrame,\n",
    "                          attack_type:str,\n",
    "                          num_samples:int=25):\n",
    "    ATTACK_TYPE_SAMPLES = df[df['Attack type']==attack_type].sample(num_samples)\n",
    "    ATTACK_TYPE_SAMPLES.columns = ATTACK_TYPE_SAMPLES.columns.str.strip()\n",
    "    ATTACK_TYPE_SAMPLES.reset_index(drop=True, inplace=True)\n",
    "    ATTACK_TYPE_SAMPLES.drop(columns=[\"Attack type\"], inplace=True)\n",
    "    attack_type_md = ATTACK_TYPE_SAMPLES.to_markdown()\n",
    "    return attack_type_md\n",
    "\n",
    "desc_dict = {\n",
    "    \"id\": \"A unique ID to distinguish the sensor node in any round and at any stage. For example, node number 25 in the third round and in the first stage is to be symbolized as 001 003 025.\",\n",
    "    \"Time\": \"The current simulation time of the node.\",\n",
    "    \"Is_CH\": \"A flag to distinguish whether the node is CH with value 1 or normal node with value 0.\",\n",
    "    \"who CH\": \"The ID of the CH in the current round.\",\n",
    "    \"Dist_To_CH\": \"The distance between the node and its CH in the current round.\",\n",
    "    \"ADV_S\": \"The number of advertise CH’s broadcast messages sent to the nodes.\",\n",
    "    \"ADV_R\": \"The number of advertise CH messages received from CHs.\",\n",
    "    \"JOIN_S\": \"The number of join request messages sent by the nodes to the CH.\",\n",
    "    \"JOIN_R\": \"The number of join request messages received by the CH from the nodes.\",\n",
    "    \"SCH_S\": \"The number of advertise TDMA schedule broadcast messages sent to the nodes.\",\n",
    "    \"SCH_R\": \"The number of TDMA schedule messages received from CHs.\",\n",
    "    \"Rank\": \"The order of this node within the TDMA schedule.\",\n",
    "    \"DATA_S\": \"The number of data packets sent from a sensor to its CH.\",\n",
    "    \"DATA_R\": \"The number of data packets received from CH.\",\n",
    "    \"Data_Sent_To_BS\": \"The number of data packets sent to the BS.\",\n",
    "    \"dist_CH_To_BS\": \"The distance between the CH and the BS.\",\n",
    "    \"send_code\": \"The cluster sending code.\",\n",
    "    \"Expaned Energy\": \"The amount of energy consumed in the previous round.\",\n",
    "}\n",
    "description_md = \"| Key           | Description |\\n\"\n",
    "description_md += \"|-------------------|-----------------|\\n\"\n",
    "for key, description in desc_dict.items():\n",
    "    description_md += f\"| {key} | {description} |\\n\"\n",
    "    \n",
    "returns_format = \"\"\"```python\n",
    "{\n",
    "    'id': value,\n",
    "    'Time': value,\n",
    "    'Is_CH': value,\n",
    "    'who CH': value,\n",
    "    'Dist_To_CH': value,\n",
    "    'ADV_S': value,\n",
    "    'ADV_R': value,\n",
    "    'JOIN_S': value,\n",
    "    'JOIN_R': value,\n",
    "    'SCH_S': value,\n",
    "    'SCH_R': value,\n",
    "    'Rank': value,\n",
    "    'DATA_S': value,\n",
    "    'DATA_R': value,\n",
    "    'Data_Sent_To_BS': value,\n",
    "    'dist_CH_To_BS': value,\n",
    "    'send_code': value,\n",
    "    'Expaned Energy': value\n",
    "}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = \"\"\"\n",
    "당신은 데이터 생성 전문가이다.\n",
    "\n",
    "WSN-DS (Wireless Sensor Network Detection System) 데이터셋에서\n",
    "Blackhole 공격 유형에 대해 데이터를 생성하고자 한다.\n",
    "\n",
    "아래 [Description]과 [Examples]를 참고하여 데이터를 한개 생성하라. \n",
    "\n",
    "[Description]\n",
    "{description_md}\n",
    "\n",
    "[Examples]\n",
    "{attack_type_md}\n",
    "\n",
    "데이터는 딕셔너리 형태로, examples 예시와 동일한 key, value 형태를 참고해서 아래 [Returns] 형태로 생성하라. \n",
    "\n",
    "[Returns]\n",
    "{returns_format}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_TYPES = ['Grayhole', 'Blackhole', 'TDMA', 'Flooding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsn_df = pd.read_csv(\"WSN-DS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_attack_type_samples = []\n",
    "# for attack_type in ATTACK_TYPES:\n",
    "#     for i in tqdm(range(150)):\n",
    "#         PROMPT = BASE_PROMPT.format(description_md=description_md, \n",
    "#                                     attack_type_md=get_attack_type_samples(df=wsn_df, \n",
    "#                                                                            attack_type=attack_type, \n",
    "#                                                                            num_samples=25), \n",
    "#                                     returns_format=returns_format)\n",
    "#         result = LLM_Call(PROMPT)\n",
    "#         data_dict = extract_data_dict(result)\n",
    "#         if data_dict is not None:\n",
    "#             data_dict['Attack type'] = attack_type\n",
    "#             new_attack_type_samples.append(data_dict)\n",
    "#         else:\n",
    "#             print(f\"Failed to extract data_dict from result {i}\")\n",
    "# new_attack_type_samples = pd.DataFrame(new_attack_type_samples)\n",
    "# new_attack_type_samples.to_csv(\"new_attack_type_samples_ver2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = wsn_df[wsn_df['Attack type']==\"Normal\"].sample(400,random_state=42)\n",
    "normal_df.columns = normal_df.columns.str.strip()\n",
    "concat_original_df = copy.deepcopy(normal_df)\n",
    "for attack_type in ATTACK_TYPES:\n",
    "    attack_type_df = wsn_df[wsn_df['Attack type']==attack_type].sample(200,random_state=42)\n",
    "    attack_type_df.columns = attack_type_df.columns.str.strip()\n",
    "    concat_original_df = pd.concat([concat_original_df, attack_type_df], ignore_index=True)\n",
    "    \n",
    "new_attack_type_df = pd.read_csv(f\"augmented_with_LLM.csv\")\n",
    "new_attack_type_df.columns = new_attack_type_df.columns.str.strip()\n",
    "concat_all_df = pd.concat([concat_original_df,new_attack_type_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM-based Augmentation Result]\n",
      "Accuracy: 0.9075\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Blackhole       0.75      0.96      0.84        85\n",
      "    Flooding       1.00      0.99      0.99        75\n",
      "    Grayhole       0.95      0.67      0.79        85\n",
      "      Normal       0.95      0.99      0.97        83\n",
      "        TDMA       0.97      0.94      0.96        72\n",
      "\n",
      "    accuracy                           0.91       400\n",
      "   macro avg       0.92      0.91      0.91       400\n",
      "weighted avg       0.92      0.91      0.91       400\n",
      "\n",
      "[SMOTE-based Augmentation Result]\n",
      "Accuracy: 0.91\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Blackhole       0.71      0.97      0.82        69\n",
      "    Flooding       1.00      1.00      1.00        78\n",
      "    Grayhole       0.99      0.71      0.83        94\n",
      "      Normal       0.94      0.98      0.96        83\n",
      "        TDMA       0.96      0.93      0.95        76\n",
      "\n",
      "    accuracy                           0.91       400\n",
      "   macro avg       0.92      0.92      0.91       400\n",
      "weighted avg       0.93      0.91      0.91       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_sample_based_LLM = concat_all_df.drop(columns=[\"Attack type\"])\n",
    "Y_sample_based_LLM = concat_all_df[\"Attack type\"]\n",
    "# print(X_sample_based_LLM.shape, Y_sample_based_LLM.shape)\n",
    "X_train_based_LLM, X_test_based_LLM, y_train_based_LLM, y_test_based_LLM = train_test_split(X_sample_based_LLM, Y_sample_based_LLM, test_size=0.2, random_state=42)\n",
    "scaler_resampling = StandardScaler()\n",
    "X_train_scaled = scaler_resampling.fit_transform(X_train_based_LLM)\n",
    "X_test_scaled = scaler_resampling.transform(X_test_based_LLM)\n",
    "logreg_resampling = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg_resampling.fit(X_train_scaled, y_train_based_LLM)\n",
    "y_pred_based_LLM = logreg_resampling.predict(X_test_scaled)\n",
    "print(\"[LLM-based Augmentation Result]\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_based_LLM, y_pred_based_LLM))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_based_LLM, y_pred_based_LLM))\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_sample_based_SMOTE = concat_original_df.drop(columns=[\"Attack type\"])\n",
    "Y_sample_based_SMOTE = concat_original_df[\"Attack type\"]\n",
    "X_SMOTE, Y_SMOTE = smote.fit_resample(X_sample_based_SMOTE, Y_sample_based_SMOTE)\n",
    "original_data_size = len(X_sample_based_SMOTE)\n",
    "X_augmented = X_SMOTE[original_data_size:]  # 증강된 데이터만 선택\n",
    "Y_augmented = Y_SMOTE[original_data_size:]  # 증강된 라벨만 선택\n",
    "smote_df = pd.DataFrame(X_augmented, columns=X_sample_based_SMOTE.columns)\n",
    "smote_df[\"Attack type\"] = Y_augmented\n",
    "smote_df.to_csv(\"augmented_with_smote_only.csv\", index=False)\n",
    "\n",
    "X_train_based_SMOTE, X_test_based_SMOTE, y_train_based_SMOTE, y_test_based_SMOTE = train_test_split(X_SMOTE, Y_SMOTE, test_size=0.2, random_state=42)\n",
    "scaler_resampling = StandardScaler()\n",
    "X_train_scaled = scaler_resampling.fit_transform(X_train_based_SMOTE)\n",
    "X_test_scaled = scaler_resampling.transform(X_test_based_SMOTE)\n",
    "logreg_resampling = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg_resampling.fit(X_train_scaled, y_train_based_SMOTE)\n",
    "y_pred_based_SMOTE = logreg_resampling.predict(X_test_scaled)\n",
    "print(\"[SMOTE-based Augmentation Result]\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_based_SMOTE, y_pred_based_SMOTE))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_based_SMOTE, y_pred_based_SMOTE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"WSN-DS.csv\")\n",
    "original_df.columns = original_df.columns.str.strip()\n",
    "augmented_with_LLM_df = pd.read_csv(\"augmented_with_LLM.csv\")\n",
    "augmented_with_LLM_df.columns = augmented_with_LLM_df.columns.str.strip()\n",
    "augmented_with_smote_df = pd.read_csv(\"augmented_with_smote_only.csv\")\n",
    "augmented_with_smote_df.columns = augmented_with_smote_df.columns.str.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
